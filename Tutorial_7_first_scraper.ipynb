{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Web Scraping 1: A first webscraper\n",
    "This tutorial will guide you through the basics of web scraping using Python. You’ll learn how to:\n",
    "\n",
    "- Request HTML from a webpage\n",
    "- Use BeautifulSoup to parse the HTML\n",
    "- Extract specific content, like titles and paragraphs\n",
    "\n",
    "By the end of this tutorial, you’ll be able to create a working web scraper and to understand how to handle errors when scraping.\n",
    "\n",
    "\n",
    "## What is web scraping about?\n",
    "\n",
    "Web scraping is the process of automatically extracting data from websites. With Python, we can do this efficiently using libraries like `urllib` and `BeautifulSoup`.\n",
    "\n",
    "The basic workflow for scraping a web page consists of:\n",
    "\n",
    "- Sending a request to the server to retrieve the web page.\n",
    "- Reading the content of the page.\n",
    "- Parsing the HTML to extract the desired information.\n",
    "\n",
    "Let’s break down the process with an example.\n",
    "\n",
    "Note on the Jupyter environment:\n",
    "- Markdown cells like this are used for explanations.\n",
    "- Code cells (gray boxes as the ones below) are where you write and run Python code. You can add the desired code to the cell and run all the code in the cell by clicking on the Run button (icon below the menu) or directly on the keyboard with the Shift + Enter keys.\n",
    "- To clear the results after execution of a code, click Cell>Current Outputs>Clear in the menu.\n",
    "\n",
    "\n",
    "\n",
    "### Step 1: Fetching HTML content using urlopen\n",
    "The `urlopen` function from Python’s `urllib.request` module allows us to send an HTTP request and retrieve the HTML content of a page. Try to retrieve the content of “page1” by executing the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# Send a request to the server and fetch the HTML content of the page\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "\n",
    "# Print the raw HTML content\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "`urlopen` sends a request to the provided URL and retrieves the web page.\n",
    "html.read() gives us the HTML content, which is currently in its raw form (a long string of HTML code).\n",
    "\n",
    "### Step 2: Parsing HTML with BeautifulSoup\n",
    "The BeautifulSoup library makes it easy to parse and navigate HTML or XML files. Let’s use it to extract specific data from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the HTML content of the page\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "# Extract and print the content of the <h1> tag\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do you find in the variable `bs`?\n",
    "\n",
    "**Explanation:**\n",
    "- `BeautifulSoup` is used to parse the raw HTML content and convert it into a structured format.\n",
    "- `bs.h1` accesses the first `<h1>` tag on the page.\n",
    "\n",
    "## Exercise 1: Modify the scraping code\n",
    "In this exercise, you’ll build on the existing code by:\n",
    "\n",
    "- Changing the URL to scrape a different page (try using Wikipedia).\n",
    "- Modify the code to extract the content of the `<title>` tag instead of the `<h1>` tag.\n",
    "- Print the first `<p>` tag as well.\n",
    "\n",
    "*Hint:* You can access the title tag using `bs.title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the HTML content of a different page (modify the URL as needed)\n",
    "html = urlopen('http://example.com')\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "print(bs)\n",
    "\n",
    "# Print the content of the <title> tag\n",
    "print(\"Title:\", bs.title.get_text())\n",
    "\n",
    "# Print the content of the first <p> tag\n",
    "print(\"First paragraph:\", bs.p.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining errors and error handling\n",
    "When scraping websites, it's common to encounter errors:\n",
    "\n",
    "- HTTPError: Occurs when the webpage can’t be found (e.g., a 404 error).\n",
    "- URLError: Occurs when the server is down or the URL is incorrect.\n",
    "\n",
    "Let’s explore how to handle these errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "# Error handling with try-except blocks\n",
    "try:\n",
    "    html = urlopen(\"https://pythonscrapingthisurldoesnotexist.com\")\n",
    "except HTTPError as e:\n",
    "    print(\"The server returned an HTTP error:\", e)\n",
    "except URLError as e:\n",
    "    print(\"The server could not be found:\", e)\n",
    "else:\n",
    "    print(\"HTML content retrieved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing functions to reuse code\n",
    "To make your scraping process more efficient, you can write functions that encapsulate the scraping logic. This way, you can reuse the same code for multiple pages without rewriting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to retrieve the <h1> title of a webpage\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "# Call the function with a URL\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "    print(\"Title could not be found\")\n",
    "else:\n",
    "    print(\"Title:\", title.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Scrape a Wikipedia page\n",
    "Now that you know the basics of error handling and how to use BeautifulSoup, let’s scrape a more complex page!\n",
    "\n",
    "Task:\n",
    "\n",
    "- Use the URL for any Wikipedia page (e.g., Split screen) and scrape the title.\n",
    "- Extract and print the first paragraph `<p>` tag on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your task: Modify the code below to scrape a new URL and extract the <title> and <p> tags\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the HTML content of a Wikipedia page (modify the URL as needed)\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Split_screen_(video_production)')\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "# Print the content of the <title> tag\n",
    "print(\"Title:\", bs.title.get_text())\n",
    "\n",
    "# Print the content of the first <p> tag\n",
    "print(\"First paragraph:\", bs.p.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this tutorial, you’ve learned:\n",
    "\n",
    "- How to make web requests using `urlopen`.\n",
    "- How to parse HTML with `BeautifulSoup`.\n",
    "- How to handle common web scraping errors.\n",
    "- How to extract specific content, like titles and paragraphs.\n",
    "\n",
    "Feel free to experiment with scraping different websites and extracting different types of information!\n",
    "\n",
    "### Additional exercises\n",
    "- Modify your scraper to find all image tags on a page using `bs.find_all('img')` and print their URLs.\n",
    "- Try scraping a different Wikipedia page or another website of your choice.\n",
    "- Write a function to check if an element exists before trying to scrape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
