{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Web Scraping 2: Navigating HTML Trees\n",
    "\n",
    "In this tutorial, we will introduce more advanced concepts for web scraping. You will learn how to efficiently navigate through HTML structures, target specific elements based on their attributes, and filter the content to scrape. \n",
    "\n",
    "You will be introduced to **tree navigation** concepts in HTML, which will allow you to move between elements on a webpage and scrape information based on relationships between elements like **parents**, **children**, and **siblings**.\n",
    "\n",
    "---\n",
    "\n",
    "## How to search for specific elements with advanced tools?\n",
    "\n",
    "\n",
    "1. **Extracting HTML by tag attributes**: Previous information extractions have been conducted using simple tags. However, it is also possible to filter tags according to their **attributes** such as `class`, `id`, or custom attributes.\n",
    "   \n",
    "2. **Tree navigation in HTML**: As an alternative to extraction by attributes whose names are given, extraction can be done by navigating **up** (parents), **down** (children) and **down** (siblings) in an HTML document, allowing extraction of nested or related content.\n",
    "\n",
    "3. **Regular Expressions with BeautifulSoup**: Another alternative to attribute search is the use of **regular expressions (regex)**, to extract more flexible data from complex structures. Here, the task is to identify specific string patterns, such as all words beginning with the letter 'a' or the string 'upjv'.\n",
    "---\n",
    "\n",
    "## 1. Search by attribute using `findAll`\n",
    "\n",
    "### Step 1: Fetching the HTML content\n",
    "\n",
    "As in Tutorial 1, we start by fetching the HTML content of a page. The `urlopen` function will retrieve the raw HTML, which will be parsed by **BeautifulSoup**.\n",
    "\n",
    "**Task:** Run the code below to view the HTML structure of the webpage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** The code fetches and prints the entire HTML structure of the target page. This is useful for exploring the layout of the page and identifying what elements we want to scrape.\n",
    "\n",
    "### Step 2: Extracting specific tags with attributes\n",
    "Next, we move beyond extracting simple tags and target tags with specific attributes like class or id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#.find_all('span', {'class':{'green', 'red'}})\n",
    "nameList = bs.findAll('span', {'class': ['green', 'red']})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** This code filters `<span>` tags based on their class attribute, extracting only those that are colored green or red.\n",
    "    \n",
    "**Task:** Guess what the codes below will give, and then modify the code to extract all the `<span` tags that have a different class or another attribute of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameList = bs.findAll('span', {'class': 'green'})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = bs.find_all('span', {'class':{'green', 'red'}})\n",
    "print([text for text in allText])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Finding all titles (headings)\n",
    "Let's move on to extracting all the heading tags (`<h1>` to `<h6>`). This is useful when scraping articles or web pages with structured headings.\n",
    "\n",
    "**Task:**\n",
    "Run the code and observe the output. What headings were extracted from the page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print([title for title in titles])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Counting occurrences of text\n",
    "\n",
    "Sometimes, we want to count how many times a specific piece of text appears on the page. For example, let's count how many times the phrase \"the prince\" appears on the page.\n",
    "\n",
    "**Task:**\n",
    "Modify this code to count the occurrences of any other word you find on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameList = bs.find_all(text='the prince')\n",
    "print(len(nameList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Counting words\n",
    "Write a code that counts the number of occurrences of a list of words on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search by type using `find`\n",
    "\n",
    "### Step 1: Extracting table data\n",
    "We can extract data from HTML tables as well. In this example, let's scrape data from a table on another webpage.\n",
    "\n",
    "**Task:**\n",
    "Modify this code to extract only the actual data rows (not the header) from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for child in bs.find('table',{'id':'giftList'}).children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extracting sibling / parent elements\n",
    "Let's say you want to extract elements that are next to each other in the HTML structure (i.e., siblings). Here's how to do it.\n",
    "\n",
    "**Task:**\n",
    "Try extracting only the product data without the title row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:\n",
    "    print(sibling) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you might need to navigate up the tree to find the parent of a specific element. Here's how to do that.\n",
    "\n",
    "**Task:**\n",
    "Modify the code to extract the text associated with another image from the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.find('img',\n",
    "              {'src':'../img/gifts/img1.jpg'})\n",
    "      .parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Search by expression \n",
    "\n",
    "### Step 1: Regular expressions \n",
    "Regular expressions (regex) allow you to search for patterns in text. This is useful for finding elements with complex or unpredictable attributes.\n",
    "\n",
    "**Task:** What type of files does this code retrieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "images = bs.find_all('img', {'src':re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
    "for image in images: \n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Lambda functions\n",
    "\n",
    "You can also use lambda functions to create custom filtering criteria. For example, to find all tags that have exactly two attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.find_all(lambda tag: len(tag.attrs) == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "Modify the lambda function to find tags with three or more attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.find_all(lambda tag: tag.get_text() == 'Or maybe he\\'s only resting?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.find_all('', text='Or maybe he\\'s only resting?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Extracting prices with regex\n",
    "\n",
    "On Page 3 in Step 1 above, you found several products listed with their prices displayed in bold. For this exercise, you will:\n",
    " - Extract all the prices displayed on the page using regular expressions.\n",
    " - Display the extracted prices in a readable format.\n",
    "\n",
    "**Instructions to follow:**\n",
    " - Inspect Page 3 in your browser, and examine how the prices are structured.\n",
    " - Identify price format and propose a regular expression for this pattern: \\\\$xx.xx (e.g., \\\\$15.00*, \\\\$0.50).\n",
    "     - you can start simply using your own words to describe the pattern, then \"convert\" it into a regex.\n",
    " \n",
    "**Questions:**\n",
    "1. Run the code below and observe the output. Correct the code to get rid of the error message.\n",
    "2. After correction, does it correctly extract all the prices from the page?\n",
    "2. Modify the regex to extract (a) only standalone prices, and (b) prices with only one digit before the decimal point (e.g., \\\\$0.50, but not \\\\$15.00).\n",
    "4. Format the output to display the extracted prices with a custom message (e.g., \"The price is $0.50\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Step 1: Fetch the HTML content of the webpage\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Step 2: Use the regular expression to find all prices in the format $xx.xx\n",
    "prices = bs.find_all(text=re.compile(r'\\$\\d+\\.\\d{2}'))\n",
    "\n",
    "# Step 3: Print out the extracted prices\n",
    "for price in prices:\n",
    "    print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You’ve now learned the basics of scraping using Python and BeautifulSoup, from fetching HTML pages to navigating through complex HTML structures, handling text, tables, and images. Experiment with different web pages and see what you can extract!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
