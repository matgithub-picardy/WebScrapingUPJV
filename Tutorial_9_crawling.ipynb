{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Web Scraping 3: Advanced Web Crawling\n",
    "\n",
    "In this tutorial, we explore more advanced web scraping techniques such as crawling across multiple pages, collecting specific data from sites, and even traversing the entire internet! We will dive into how to retrieve links from a webpage, perform a random walk through articles, and collect external and internal links systematically.\n",
    "\n",
    "\n",
    "## Learning objectives\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "- Understand how to retrieve data from web pages systematically.\n",
    "- Traverse through multiple pages of a website.\n",
    "- Collect specific content such as internal and external links.\n",
    "- Crawl across multiple websites and explore the internet through web scraping.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Retrieving all links from a page\n",
    "We will start by retrieving all links from a webpage using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for link in bs.find_all('a'):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "We open the Wikipedia page for \"Kevin Bacon\".\n",
    "We use `find_all('a')` to extract all the anchor (`<a>`) tags, which usually contain links.\n",
    "\n",
    "## Exercise 1:\n",
    "Modify the code to print only the first 10 links found on the page.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Retrieving only article links\n",
    "Now, let's modify our scraper to retrieve only Wikipedia article links. We will use regular expressions to filter out non-article links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for link in bs.find('div', {'id':'bodyContent'}).find_all(\n",
    "    'a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
    "    print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** We use a re.compile regular expression to only find URLs starting with /wiki/ and exclude those with colons (:), which represent non-article pages (like categories or file pages).\n",
    "\n",
    "## Exercise 2:\n",
    "\n",
    "Modify the code to print only the first 5 article links and store them in a list.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Random walk on Wikipedia articles\n",
    "We will now implement a random walk through Wikipedia articles. This means, starting from an article (e.g., Kevin Bacon), we will follow a random link to another Wikipedia article and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "random.seed(datetime.datetime.now().strftime('%s'))\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen(f'http://en.wikipedia.org{articleUrl}')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    return bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "while len(links) > 0:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Explanation:** We use a random function to select a random Wikipedia article from the list of article links.\n",
    "The program continues to walk randomly from one article to another.\n",
    "\n",
    "## Exercise 3:\n",
    "\n",
    "Set a limit to only walk through 5 Wikipedia articles and print the final article visited.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Recursively crawling an entire site\n",
    "Now let's implement a recursive crawler that will traverse through all Wikipedia pages it encounters and collect all internal links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pages = set()\n",
    "def getLinks(pageUrl):\n",
    "    html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "getLinks('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** We keep track of all pages visited in the pages set to avoid duplication.\n",
    "The function recursively calls itself, crawling through every internal Wikipedia page it encounters.\n",
    "\n",
    "## Exercise 4:\n",
    "\n",
    "Limit the recursion to crawl only 3 pages deep and stop after reaching 20 unique pages.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Collecting data from a Wikipedia site\n",
    "We will modify the crawler to not only traverse pages but also collect useful data from them such as the title, the first paragraph, and the edit link (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pages = set()\n",
    "def getLinks(pageUrl):\n",
    "    html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        print(bs.h1.get_text())\n",
    "        #mw-parser-output\n",
    "        bodyContent = bs.find('div', {'id':'bodyContent'}).find_all('p')\n",
    "        if len(bodyContent):\n",
    "            print(bodyContent[0])\n",
    "        print(bs.find(id='ca-edit').find('a').attrs['href'])\n",
    "    except AttributeError:\n",
    "        print('This page is missing something! Continuing.')\n",
    "    \n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print('-'*20)\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "getLinks('/wiki/General-purpose_programming_language') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5:\n",
    "\n",
    "Modify the function to also collect the last paragraph of the article, if available.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Crawling across the Internet\n",
    "Letâ€™s move beyond Wikipedia and build a crawler that can traverse external sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "#Retrieves a list of all Internal links found on a page\n",
    "def getInternalLinks(bs, url):\n",
    "    netloc = urlparse(url).netloc\n",
    "    scheme = urlparse(url).scheme\n",
    "    internalLinks = set()\n",
    "    for link in bs.find_all('a'):\n",
    "        if not link.attrs.get('href'):\n",
    "            continue\n",
    "        parsed = urlparse(link.attrs['href'])\n",
    "        if parsed.netloc == '':\n",
    "            internalLinks.add(f'{scheme}://{netloc}/{link.attrs[\"href\"].strip(\"/\")}')\n",
    "        elif parsed.netloc == netloc:\n",
    "            internalLinks.add(link.attrs['href'])\n",
    "    return list(internalLinks)\n",
    "            \n",
    "#Retrieves a list of all external links found on a page\n",
    "def getExternalLinks(bs, url):\n",
    "    netloc = urlparse(url).netloc\n",
    "    externalLinks = set()\n",
    "    for link in bs.find_all('a'):\n",
    "        if not link.attrs.get('href'):\n",
    "            continue\n",
    "        parsed = urlparse(link.attrs['href'])\n",
    "        if parsed.netloc != '' and parsed.netloc != netloc:\n",
    "            externalLinks.add(link.attrs['href'])\n",
    "    return list(externalLinks)\n",
    "\n",
    "def getRandomExternalLink(startingPage):\n",
    "    bs = BeautifulSoup(urlopen(startingPage), 'html.parser')\n",
    "    externalLinks = getExternalLinks(bs, startingPage)\n",
    "    if not len(externalLinks):\n",
    "        print('No external links, looking around the site for one')\n",
    "        internalLinks = getInternalLinks(bs, startingPage)\n",
    "        return getRandomExternalLink(random.choice(internalLinks))\n",
    "    else:\n",
    "        return random.choice(externalLinks)\n",
    "    \n",
    "def followExternalOnly(startingSite):\n",
    "    externalLink = getRandomExternalLink(startingSite)\n",
    "    print(f'Random external link is: {externalLink}')\n",
    "    followExternalOnly(externalLink)\n",
    "\n",
    "\n",
    "followExternalOnly('https://www.oreilly.com/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You now have the tools to build more advanced web crawlers that can traverse single domains or even multiple websites. This tutorial also demonstrated how to use regular expressions and Python's random library for web scraping and exploration.\n",
    "\n",
    "Be mindful of the ethical and legal considerations discussed in earlier tutorials when building and deploying web crawlers.\n",
    "\n",
    "## Exercise 6:\n",
    "Build a crawler that logs every external link it finds from the homepage of a website and prints the total count of unique external links discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects a list of all external URLs found on the site\n",
    "allExtLinks = []\n",
    "allIntLinks = []\n",
    "\n",
    "\n",
    "def getAllExternalLinks(url):\n",
    "    bs = BeautifulSoup(urlopen(url), 'html.parser')\n",
    "    internalLinks = getInternalLinks(bs, url)\n",
    "    externalLinks = getExternalLinks(bs, url)\n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.append(link)\n",
    "            print(link)\n",
    "\n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            allIntLinks.append(link)\n",
    "            getAllExternalLinks(link)\n",
    "\n",
    "\n",
    "allIntLinks.append('https://oreilly.com')\n",
    "getAllExternalLinks('https://www.oreilly.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
